{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
    "session = tf.Session(config=config)\n",
    "KTF.set_session(session)\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Flatten, Input, concatenate\n",
    "from keras.layers import Input, TimeDistributed, Dense, Lambda, concatenate, Dropout, BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Nadam\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line = train[:10]\n",
    "# line['hour'] = line['hour'].astype(str)\n",
    "# line['Source_Station_encode'] = line['Source_Station_encode'].astype('int')\n",
    "# line['Target_Station_encode'] = line['Target_Station_encode'].astype('int')\n",
    "# line['Source_Station_encode'] = line['Source_Station_encode'] + 1\n",
    "# line['Target_Station_encode'] = line['Target_Station_encode'] + 1\n",
    "# line['Source_Station_encode'] = line['Source_Station_encode'].astype('str')\n",
    "# line['Target_Station_encode'] = line['Target_Station_encode'].astype('str')\n",
    "# line['hour_s'] = line['hour'] + line['Source_Station_encode']\n",
    "# line['hour_e'] = line['hour'] + line['Target_Station_encode']\n",
    "# line['hour_s'] = line['hour_s'].astype(int)\n",
    "# line['hour_e'] = line['hour_e'].astype(int)\n",
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zuhe(line):\n",
    "    line['hour'] = line['hour'].astype(str)\n",
    "    line['weekday'] = line['weekday'].astype(str)\n",
    "    line['is_peek'] = line['is_peek'].astype(str)\n",
    "    line['is_workday'] = line['is_workday'].astype(str)\n",
    "                                   \n",
    "    line['hour_weekday'] = line['hour'] + line['weekday']\n",
    "    line['peak_workday'] = line['is_peek'] + line['is_workday']\n",
    "    \n",
    "    line['hour'] = line['hour'].astype(int)\n",
    "    line['is_peek'] = line['is_peek'].astype(int)\n",
    "    line['is_workday'] = line['is_workday'].astype(int)\n",
    "    line['weekday'] = line['weekday'].astype(int)\n",
    "    \n",
    "    line['hour_weekday'] = line['hour_weekday'].astype(int)\n",
    "    line['peak_workday'] = line['peak_workday'].astype(int)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:13: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  del sys.path[0]\n",
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  app.launch_new_instance()\n",
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14223933,)\n"
     ]
    }
   ],
   "source": [
    "with open('./data/train-id4-crowd-grid6.txt', 'rb') as data_file:\n",
    "    train = pickle.load(data_file)\n",
    "    #训练验证都划分\n",
    "    #900 29 26 #800 27.9 25.9 #700 27.5 25.3 #27.4 24.8 #26.4 24.08 #23 21 #(90)250 21 19.5 #(80)17. 16.5\n",
    "    #训练划分 验证不划分\n",
    "    #250\n",
    "train = train[(train['Diff_Time']<600)]\n",
    "train = zuhe(train)\n",
    "with open('./data/test-id4-crowd-grid6.txt', 'rb') as data_file:\n",
    "    test = pickle.load(data_file)\n",
    "test = zuhe(test)\n",
    "from sklearn import preprocessing\n",
    "x1 = train['ID'].as_matrix().reshape([-1,1])\n",
    "N1 = x1.shape[0]\n",
    "\n",
    "x2 = test['ID'].as_matrix().reshape([-1,1])\n",
    "N2 = x2.shape[0]\n",
    "x = np.concatenate((x1,x2))\n",
    "x = preprocessing.LabelEncoder().fit_transform(x) #13963746x21567\n",
    "print(x.shape)#(14454126, 21567)\n",
    "train['new_ID'] = x[:N1]\n",
    "test['new_ID'] = x[N1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_ID'] ['Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'hour_weekday', 'peak_workday'] ['Source_Station_encode'] ['Target_Station_encode'] ['new_s_t'] ['new_s_t_o']\n",
      "train (13733553, 1) (13733553, 15) (13733553, 1) (13733553, 1) (13733553, 1) (13733553, 1)\n"
     ]
    }
   ],
   "source": [
    "col1 = ['new_ID']\n",
    "col2 = [c for c in train if\n",
    "       c not in ['Unnamed: 0', 'is_crowd', 'grid_aver_diff', 'grid_aver_d', 'Source_Station_encode', 'Target_Station_encode', 's_t', 'new_s_t', 's_t_o', 'new_s_t_o','ID2', 's_ij', 'e_ij', 's_x', 's_y', 'e_x', 'e_y', 'new_ID','ID','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station', 'O_TIME', 'aver_v', 'max_v',\n",
    "                 'Diff_Time']]\n",
    "col3 = ['Source_Station_encode']\n",
    "col4 = ['Target_Station_encode']\n",
    "col5 = ['new_s_t']\n",
    "col6 = ['new_s_t_o']\n",
    "\n",
    "print(col1,col2,col3,col4,col5,col6)\n",
    "X_train1 = train[col1].values\n",
    "X_train2 = train[col2].values\n",
    "X_train3 = train[col3].values\n",
    "X_train4 = train[col4].values\n",
    "X_train5 = train[col5].values\n",
    "X_train6 = train[col6].values\n",
    "y_train = train['Diff_Time'].values\n",
    "print('train',X_train1.shape,X_train2.shape,X_train3.shape,X_train4.shape,X_train5.shape,X_train6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_ID'] ['Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'hour_weekday', 'peak_workday'] ['Source_Station_encode'] ['Target_Station_encode'] ['new_s_t'] ['new_s_t_o']\n",
      "test (490380, 1) (490380, 15) (490380, 1) (490380, 1) (490380, 1) (490380, 1)\n"
     ]
    }
   ],
   "source": [
    "col1 = ['new_ID']\n",
    "col2 = [c for c in test if\n",
    "       c not in ['Unnamed: 0', 'is_crowd', 'grid_aver_diff', 'grid_aver_d', 'Source_Station_encode', 'Target_Station_encode', 's_t', 'new_s_t', 's_t_o', 'new_s_t_o','ID2', 's_ij', 'e_ij', 's_x', 's_y', 'e_x', 'e_y', 'new_ID','ID','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station', 'O_TIME', 'aver_v', 'max_v',\n",
    "                 'Diff_Time','Distance1', 'distance2','TERMINALNO', 'new_dist']]\n",
    "col3 = ['Source_Station_encode']\n",
    "col4 = ['Target_Station_encode']\n",
    "col5 = ['new_s_t']\n",
    "col6 = ['new_s_t_o']\n",
    "\n",
    "print(col1,col2,col3,col4,col5,col6)\n",
    "X_test1 = test[col1].values\n",
    "X_test2 = test[col2].values\n",
    "X_test3 = test[col3].values\n",
    "X_test4 = test[col4].values\n",
    "X_test5 = test[col5].values\n",
    "X_test6 = test[col6].values\n",
    "print('test',X_test1.shape,X_test2.shape,X_test3.shape,X_test4.shape,X_test5.shape,X_test6.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean((y_pred - y_true)**2))\n",
    "    \n",
    "def mse_loss(y_true,y_pred):\n",
    "    return K.sqrt(K.mean((y_pred - y_true)**2))\n",
    "\n",
    "def get_model():\n",
    "    input1 = Input(shape=(1,))\n",
    "    input2 = Input(shape=(15,))\n",
    "    input3 = Input(shape=(1,))\n",
    "    input4 = Input(shape=(1,))\n",
    "    input5 = Input(shape=(1,))\n",
    "#     input6 = Input(shape=(1,))\n",
    "\n",
    "    x1 = Embedding(21064, 128, input_length=1)(input1)\n",
    "    x1 = Flatten()(x1)\n",
    "    x1 = BatchNormalization()(x1)\n",
    "    \n",
    "    x2 = BatchNormalization()(input2)\n",
    "    x2 = Dense(units=128,activation='relu')(x2)\n",
    "    \n",
    "    x3 = Embedding(5772, 64, input_length=1)(input3)\n",
    "    x3 = BatchNormalization()(x3)\n",
    "    x3 = Flatten()(x3)\n",
    "    \n",
    "    x4 = Embedding(5772, 64, input_length=1)(input4)\n",
    "    x4 = BatchNormalization()(x4)\n",
    "    x4 = Flatten()(x4)\n",
    "    \n",
    "    x5 = Embedding(14144, 64, input_length=1)(input5)\n",
    "    x5 = BatchNormalization()(x5)\n",
    "    x5 = Flatten()(x5)\n",
    "    \n",
    "#     x6 = Embedding(21219, 64, input_length=1)(input6)\n",
    "#     x6 = BatchNormalization()(x6)\n",
    "#     x6 = Flatten()(x6)\n",
    "\n",
    "    def dist(x):\n",
    "        return x[0]*x[1]/(K.sum(x[0]**2,axis=1,keepdims=True)+K.sum(x[1]**2,axis=1,keepdims=True))    \n",
    "    #     Aggregate\n",
    "    x3_x4 = Lambda(dist)([x3,x4])\n",
    "\n",
    "    x = concatenate([x1, x2, x3, x4, x5, x3_x4])\n",
    "#     x = Flatten()(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    out = Dense(1, activation='relu')(x)\n",
    "    \n",
    "    model = Model(inputs=[input1, input2, input3, input4, input5], outputs=out)\n",
    "    model.compile(loss=mse_loss, optimizer=Nadam(), metrics=[root_mean_squared_error])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "def get_stacking(X_train1, X_train2, X_train3, X_train4, X_train5, X_train6, labels, n_folds=10):\n",
    "    train_num, test_num = X_train1.shape[0], X_test1.shape[0]\n",
    "    second_level_train_set = np.zeros((train_num,))\n",
    "    second_level_test_set = np.zeros((test_num,))\n",
    "    test_nfolds_sets = np.zeros((test_num, n_folds))\n",
    "    \n",
    "    folds = KFold(n_splits=n_folds, shuffle=True, random_state=1001001)\n",
    "        \n",
    "    for n_fold,(train_idx, val_idx) in enumerate(folds.split(labels)):\n",
    "        train_1 = X_train1[train_idx]\n",
    "        train_2 = X_train2[train_idx]\n",
    "        train_3 = X_train3[train_idx]\n",
    "        train_4 = X_train4[train_idx]\n",
    "        train_5 = X_train5[train_idx]\n",
    "#         train_6 = X_train6[train_idx]\n",
    "        train_y = labels[train_idx]\n",
    "\n",
    "        val_1 = X_train1[val_idx]\n",
    "        val_2 = X_train2[val_idx]\n",
    "        val_3 = X_train3[val_idx]\n",
    "        val_4 = X_train4[val_idx]\n",
    "        val_5 = X_train5[val_idx]\n",
    "#         val_6 = X_train6[val_idx]\n",
    "        val_y = labels[val_idx]\n",
    "#         ckpt_path = './logtianchi/'+ model_list[n_fold]\n",
    "#         print(ckpt_path)\n",
    "        model = get_model()\n",
    "        if n_fold == 0:\n",
    "            print(model.summary())\n",
    "        print(n_fold)\n",
    "        \n",
    "\n",
    "        ckpt_path = './log/new_cv_'+str(n_fold)+'_weights-{val_loss:.4f}.hdf5'\n",
    "        \n",
    "\n",
    "        best_score = 1000\n",
    "        best_iter = 0\n",
    "        for i in range(10):\n",
    "            model.fit([train_1,train_2,train_3,train_4,train_5],train_y+np.random.uniform(-1,1,(len(train_y,))), \n",
    "                      validation_data=([val_1,val_2,val_3,val_4,val_5],val_y), \n",
    "                      epochs=1, \n",
    "                      batch_size=1024)\n",
    "            score = model.evaluate([val_1,val_2,val_3,val_4,val_5],val_y,batch_size=1024)[0]\n",
    "            print(score)\n",
    "            if score<best_score:\n",
    "                best_score = score\n",
    "                best_iter = i\n",
    "                model.save_weights('./log/new_cv'+str(n_fold)+'.h5')\n",
    "            if i-best_iter>3:\n",
    "                break\n",
    "        model.load_weights('./log/new_cv'+str(n_fold)+'.h5')\n",
    "        second_level_train_set[val_idx] = model.predict([val_1, val_2,val_3,val_4,val_5], batch_size=1024)[:, 0]\n",
    "        test_nfolds_sets[:,n_fold] = model.predict([X_test1, X_test2, X_test3, X_test4, X_test5,], batch_size=1024)[:, 0]\n",
    "    \n",
    "    \"---save---\"\n",
    "    with open('./data/new_test_nfolds_sets3.txt', 'wb') as data_file:\n",
    "        pickle.dump(test_nfolds_sets, data_file)\n",
    "\n",
    "    second_level_test_set[:] = test_nfolds_sets.mean(axis=1)\n",
    "\n",
    "    result = second_level_test_set\n",
    "    \n",
    "    return second_level_train_set, second_level_test_set, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1, 64)        369408      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 1, 64)        369408      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 128)       2696192     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 1, 64)        256         embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 1, 64)        256         embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 1, 64)        905216      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 128)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15)           60          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 64)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 1, 64)        256         embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 128)          512         flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          2048        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 64)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 64)           0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           batch_normalization_1[0][0]      \n",
      "                                                                 dense_1[0][0]                    \n",
      "                                                                 flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          262656      dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 512)          262656      dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 512)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            513         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 5,132,093\n",
      "Trainable params: 5,131,423\n",
      "Non-trainable params: 670\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "0\n",
      "Train on 12360197 samples, validate on 1373356 samples\n",
      "Epoch 1/1\n",
      " 2534400/12360197 [=====>........................] - ETA: 8:56 - loss: 87.2015 - root_mean_squared_error: 87.2015"
     ]
    }
   ],
   "source": [
    "train_sets = []\n",
    "test_sets = []\n",
    "train_set, test_set, result = get_stacking(X_train1, X_train2, X_train3, X_train4, X_train5, X_train6, y_train, n_folds=10)\n",
    "train_sets.append(train_set)\n",
    "test_sets.append(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13733553, 1) (490380, 1)\n"
     ]
    }
   ],
   "source": [
    "meta_train = np.concatenate([result_set.reshape(-1,1) for result_set in train_sets], axis=1)\n",
    "meta_test = np.concatenate([y_test_set.reshape(-1,1) for y_test_set in test_sets], axis=1)\n",
    "print(meta_train.shape, meta_test.shape)\n",
    "\n",
    "\"---save---\"\n",
    "with open('./data/meta_train_nn_qrf_2.txt', 'wb') as data_file:\n",
    "    pickle.dump(meta_train, data_file)\n",
    "with open('./data/meta_test_nn_qrf_2.txt', 'wb') as data_file:\n",
    "    pickle.dump(meta_test, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([155.27383423, 225.86791992,  93.46902466, ..., 134.73045349,\n",
       "       368.41989136, 125.18015289])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([109.02749786, 129.785672  , 133.44480896, ...,  90.29129028,\n",
       "        62.36225891, 198.12910919])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zuhe(line):\n",
    "    line['hour'] = line['hour'].astype(str)\n",
    "    line['weekday'] = line['weekday'].astype(str)\n",
    "    line['s_ij'] = line['s_ij'].astype(str)\n",
    "    line['e_ij'] = line['e_ij'].astype(str)\n",
    "\n",
    "#     line['is_peek'] = line['is_peek'].astype(str)\n",
    "    line['is_crowd'] = line['is_crowd'].astype(str)\n",
    "                                   \n",
    "    line['hour_weekday'] = line['hour'] + line['weekday']\n",
    "    line['hour_crowd'] = line['hour'] + line['is_crowd']\n",
    "    line['hour_s'] = line['hour'] + line['s_ij']\n",
    "    line['hour_e'] = line['hour'] + line['e_ij']\n",
    "\n",
    "    line['hour'] = line['hour'].astype(int)\n",
    "#     line['is_peek'] = line['is_peek'].astype(int)\n",
    "    line['is_crowd'] = line['is_crowd'].astype(int)\n",
    "    line['weekday'] = line['weekday'].astype(int)\n",
    "    line['s_ij'] = line['s_ij'].astype(int)\n",
    "    line['e_ij'] = line['e_ij'].astype(int)\n",
    "    \n",
    "    line['hour_weekday'] = line['hour_weekday'].astype(int)\n",
    "    line['hour_crowd'] = line['hour_crowd'].astype(int)\n",
    "    line['hour_s'] = line['hour_s'].astype(int)\n",
    "    line['hour_e'] = line['hour_e'].astype(int)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'is_crowd', 's_ij', 'e_ij', 'grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "with open('./data/test-id4-crowd-grid4.txt', 'rb') as data_file:\n",
    "    test = pickle.load(data_file)\n",
    "test = zuhe(test)\n",
    "test = test[['ID', 'new_dist',  'O_LINENO', 'O_UP', 'Source_Station', 'Target_Station',\n",
    "       'Distance', 'distance2', 'O_TIME', 'hour', 'is_peek', 'weekday',\n",
    "       'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'max_v',\n",
    "       'h_aver_diff', 'h_aver_d', 'h_aver_v', 'TERMINALNO', \n",
    "       'is_crowd', 's_ij', 'e_ij','grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']]\n",
    "test.columns=['ID', 'Distance',  'O_LINENO', 'O_UP', 'Source_Station', 'Target_Station',\n",
    "       'Distance1', 'distance2', 'O_TIME', 'hour', 'is_peek', 'weekday',\n",
    "       'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'max_v',\n",
    "       'h_aver_diff', 'h_aver_d', 'h_aver_v', 'TERMINALNO', \n",
    "       'is_crowd', 's_ij', 'e_ij','grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n",
    "col1 = [c for c in test if\n",
    "       c not in ['Unnamed: 0','ID2' , 's_x', 's_y', 'e_x', 'e_y','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station', 'O_TIME', 'aver_v', 'max_v',\n",
    "                 'Diff_Time','Distance1', 'distance2','TERMINALNO', 'new_dist']]\n",
    "print(col1)\n",
    "\n",
    "test['pred1'] = pred\n",
    "test['pred2'] = pred\n",
    "\n",
    "sub1 = test[['O_LINENO','TERMINALNO', 'O_UP','Source_Station','Target_Station','O_TIME','pred1','pred2','Distance','Distance1']]\n",
    "sub1['O_TIME'] = pd.to_datetime(sub1['O_TIME'],format='%Y-%m-%d %H:%M:%S')\n",
    "sub1.columns = ['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime','pred1','pred2','Distance','Distance1']\n",
    "sub1 = sub1.reset_index()\n",
    "del sub1['index']\n",
    "\n",
    "sub=pd.read_csv(\"./toBePredicted_0607_segment.csv\", sep=\",\")\n",
    "sub['realTime'] = pd.to_datetime(sub['realTime'],format='%Y-%m-%d %H:%M:%S')\n",
    "sub2 = sub[['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime','distance']]\n",
    "sub2=pd.merge(sub2,sub1,on=['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime'],how='left')\n",
    "\n",
    "\n",
    "sub2['div_dist'] = sub2['Distance'] / sub2['Distance1']\n",
    "sub2['new_pred'] = sub2['div_dist'] * sub2['pred1']\n",
    "\n",
    "import math\n",
    "for i in range(sub2.shape[0]):\n",
    "    s = sub2.iloc[i]\n",
    "    if math.isnan(s['div_dist']):\n",
    "        sub2.loc[i,'new_pred'] = s['pred1']\n",
    "        \n",
    "sub2.to_csv('./qrf_toBePredicted_0807_result.csv',sep=\",\",index=False)#0605 29.2467\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
