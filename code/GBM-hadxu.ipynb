{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train-id4-crowd-grid4.txt', 'rb') as data_file:\n",
    "    line = pickle.load(data_file)\n",
    "\n",
    "with open('./data/test-id4-crowd-grid4.txt', 'rb') as data_file:\n",
    "    test = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 共享区间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>O_LINENO</th>\n",
       "      <th>O_UP</th>\n",
       "      <th>Source_Station</th>\n",
       "      <th>Target_Station</th>\n",
       "      <th>Distance</th>\n",
       "      <th>O_TIME</th>\n",
       "      <th>aver_v</th>\n",
       "      <th>hour</th>\n",
       "      <th>...</th>\n",
       "      <th>e_x</th>\n",
       "      <th>e_y</th>\n",
       "      <th>is_crowd</th>\n",
       "      <th>s_ij</th>\n",
       "      <th>e_ij</th>\n",
       "      <th>ID2</th>\n",
       "      <th>grid_aver_diff</th>\n",
       "      <th>grid_aver_d</th>\n",
       "      <th>Source_Station_encode</th>\n",
       "      <th>Target_Station_encode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>80412021</td>\n",
       "      <td>804</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>0.629329</td>\n",
       "      <td>2017-10-09 06:31:28</td>\n",
       "      <td>16.182736</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>39.14658</td>\n",
       "      <td>117.1904</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>80405690569</td>\n",
       "      <td>215.775253</td>\n",
       "      <td>0.723450</td>\n",
       "      <td>3500</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>80412122</td>\n",
       "      <td>804</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>0.983431</td>\n",
       "      <td>2017-10-09 06:35:18</td>\n",
       "      <td>15.392834</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>39.14210</td>\n",
       "      <td>117.1840</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>80405690569</td>\n",
       "      <td>241.289308</td>\n",
       "      <td>0.949461</td>\n",
       "      <td>66</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>80412223</td>\n",
       "      <td>804</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>0.690608</td>\n",
       "      <td>2017-10-09 06:36:28</td>\n",
       "      <td>35.516963</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>39.13950</td>\n",
       "      <td>117.1800</td>\n",
       "      <td>1</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>80405690569</td>\n",
       "      <td>124.409653</td>\n",
       "      <td>0.644947</td>\n",
       "      <td>61</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>80412324</td>\n",
       "      <td>804</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>0.608654</td>\n",
       "      <td>2017-10-09 06:37:38</td>\n",
       "      <td>31.302190</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>39.13580</td>\n",
       "      <td>117.1810</td>\n",
       "      <td>1</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>80405690569</td>\n",
       "      <td>162.265997</td>\n",
       "      <td>0.580119</td>\n",
       "      <td>208</td>\n",
       "      <td>2466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>80412425</td>\n",
       "      <td>804</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>0.362356</td>\n",
       "      <td>2017-10-09 06:38:28</td>\n",
       "      <td>26.089661</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>39.13380</td>\n",
       "      <td>117.1840</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>569</td>\n",
       "      <td>80405690569</td>\n",
       "      <td>84.416979</td>\n",
       "      <td>0.488454</td>\n",
       "      <td>2466</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        ID  O_LINENO  O_UP  Source_Station  Target_Station  \\\n",
       "0           1  80412021       804     1              20              21   \n",
       "1           2  80412122       804     1              21              22   \n",
       "2           3  80412223       804     1              22              23   \n",
       "3           4  80412324       804     1              23              24   \n",
       "4           5  80412425       804     1              24              25   \n",
       "\n",
       "   Distance               O_TIME     aver_v  hour          ...            \\\n",
       "0  0.629329  2017-10-09 06:31:28  16.182736     6          ...             \n",
       "1  0.983431  2017-10-09 06:35:18  15.392834     6          ...             \n",
       "2  0.690608  2017-10-09 06:36:28  35.516963     6          ...             \n",
       "3  0.608654  2017-10-09 06:37:38  31.302190     6          ...             \n",
       "4  0.362356  2017-10-09 06:38:28  26.089661     6          ...             \n",
       "\n",
       "        e_x       e_y  is_crowd  s_ij  e_ij          ID2  grid_aver_diff  \\\n",
       "0  39.14658  117.1904         0   569   569  80405690569      215.775253   \n",
       "1  39.14210  117.1840         0   569   569  80405690569      241.289308   \n",
       "2  39.13950  117.1800         1   569   569  80405690569      124.409653   \n",
       "3  39.13580  117.1810         1   569   569  80405690569      162.265997   \n",
       "4  39.13380  117.1840         0   569   569  80405690569       84.416979   \n",
       "\n",
       "   grid_aver_d  Source_Station_encode  Target_Station_encode  \n",
       "0     0.723450                   3500                     66  \n",
       "1     0.949461                     66                     61  \n",
       "2     0.644947                     61                    208  \n",
       "3     0.580119                    208                   2466  \n",
       "4     0.488454                   2466                     33  \n",
       "\n",
       "[5 rows x 35 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = set()\n",
    "for i in zip(line.Source_Station_encode, line.Target_Station_encode):\n",
    "    region.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = {v:k for k,v in enumerate(region)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10894"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_embedding = pd.read_csv('line_vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_embedding = pd.read_csv('station_vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('station_map.pkl','rb') as f:\n",
    "#     station_map = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line['Source_Station_encode'] = line.apply(lambda x:station_map.get((x['O_LINENO'], x['O_UP'], x['Source_Station']), -1), axis=1)\n",
    "# line['Target_Station_encode'] = line.apply(lambda x:station_map.get((x['O_LINENO'], x['O_UP'], x['Target_Station']), -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test['Source_Station_encode'] = test.apply(lambda x:station_map.get((x['O_LINENO'], x['O_UP'], x['Source_Station']),-1), axis=1)\n",
    "# test['Target_Station_encode'] = test.apply(lambda x:station_map.get((x['O_LINENO'], x['O_UP'], x['Target_Station']),-1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line['region_encode'] = line.apply(lambda x:region.get((x['Source_Station_encode'], x['Target_Station_encode']), -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = [c for c in line if\n",
    "       c not in ['Unnamed: 0','ID2','s_x', 's_y', 'e_x', 'e_y','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station','O_TIME', 'aver_v','max_v', 'Diff_Time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = line['Diff_Time'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "noice = np.random.randint(-3,3,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3, -1,  0, ..., -2,  1,  2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['region_encode'] = test.apply(lambda x:region.get((x['Source_Station_encode'], x['Target_Station_encode']), -1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train-id4-crowd-grid_hadxu.txt', 'wb') as data_file:\n",
    "    pickle.dump(line, data_file)\n",
    "    \n",
    "with open('./data/test-id4-crowd-grid_hadxu.txt', 'wb') as data_file:\n",
    "    pickle.dump(test, data_file)\n",
    "\n",
    "# with open('./data/train-id4-crowd-grid_qrf.txt', 'rb') as data_file:\n",
    "#     line = pickle.load(data_file)\n",
    "\n",
    "# with open('./data/test-id4-crowd-grid_qrf.txt', 'rb') as data_file:\n",
    "#     test = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = pd.merge(line,line_embedding,left_on='O_LINENO', right_on='line' )\n",
    "test = pd.merge(test,line_embedding,left_on='O_LINENO', right_on='line' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line  = downcast_dtypes(line)\n",
    "test = downcast_dtypes(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/train-id4-crowd-grid_hadxu.txt', 'wb') as data_file:\n",
    "    pickle.dump(line, data_file)\n",
    "    \n",
    "with open('./data/test-id4-crowd-grid_hadxu.txt', 'wb') as data_file:\n",
    "    pickle.dump(test, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'ID', 'O_LINENO', 'O_UP', 'Source_Station',\n",
       "       'Target_Station', 'Distance', 'distance2', 'O_TIME', 'hour', 'is_peek',\n",
       "       'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'max_v',\n",
       "       'h_aver_diff', 'h_aver_d', 'h_aver_v', 'TERMINALNO', 'new_dist', 's_x',\n",
       "       's_y', 'e_x', 'e_y', 'is_crowd', 's_ij', 'e_ij', 'ID2',\n",
       "       'grid_aver_diff', 'grid_aver_d', 'Source_Station_encode',\n",
       "       'Target_Station_encode', 'hour_weekday', 'hour_crowd', 'hour_s',\n",
       "       'hour_e'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'is_crowd', 's_ij', 'e_ij', 'grid_aver_diff', 'grid_aver_d', 'Source_Station_encode', 'Target_Station_encode', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n",
      "(13733553, 25)\n",
      "['ID', 'Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'is_crowd', 's_ij', 'e_ij', 'grid_aver_diff', 'grid_aver_d', 'Source_Station_encode', 'Target_Station_encode', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n",
      "(490380, 25)\n"
     ]
    }
   ],
   "source": [
    "def zuhe(line):\n",
    "    line['hour'] = line['hour'].astype(str)\n",
    "    line['weekday'] = line['weekday'].astype(str)\n",
    "    line['s_ij'] = line['s_ij'].astype(str)\n",
    "    line['e_ij'] = line['e_ij'].astype(str)\n",
    "\n",
    "#     line['is_peek'] = line['is_peek'].astype(str)\n",
    "    line['is_crowd'] = line['is_crowd'].astype(str)\n",
    "                                   \n",
    "    line['hour_weekday'] = line['hour'] + line['weekday']\n",
    "    line['hour_crowd'] = line['hour'] + line['is_crowd']\n",
    "    line['hour_s'] = line['hour'] + line['s_ij']\n",
    "    line['hour_e'] = line['hour'] + line['e_ij']\n",
    "\n",
    "    line['hour'] = line['hour'].astype(int)\n",
    "#     line['is_peek'] = line['is_peek'].astype(int)\n",
    "    line['is_crowd'] = line['is_crowd'].astype(int)\n",
    "    line['weekday'] = line['weekday'].astype(int)\n",
    "    line['s_ij'] = line['s_ij'].astype(int)\n",
    "    line['e_ij'] = line['e_ij'].astype(int)\n",
    "    \n",
    "    line['hour_weekday'] = line['hour_weekday'].astype(int)\n",
    "    line['hour_crowd'] = line['hour_crowd'].astype(int)\n",
    "    line['hour_s'] = line['hour_s'].astype(int)\n",
    "    line['hour_e'] = line['hour_e'].astype(int)\n",
    "    return line\n",
    "\n",
    "with open('./data/train-id4-crowd-grid_hadxu.txt', 'rb') as data_file:\n",
    "    line = pickle.load(data_file)\n",
    "    line = zuhe(line)#13963746\n",
    "line = line[(line['Diff_Time']<600)]\n",
    "line['ID'] = line['ID'].astype('category')\n",
    "line['s_ij'] = line['s_ij'].astype('category') \n",
    "line['e_ij'] = line['e_ij'].astype('category')\n",
    "\n",
    "line['Source_Station_encode'] = line['Source_Station_encode'].astype('category') \n",
    "line['Target_Station_encode'] = line['Target_Station_encode'].astype('category') \n",
    "\n",
    "train = line\n",
    "col = [c for c in train if\n",
    "       c not in ['Unnamed: 0','ID2','s_x', 's_y', 'e_x', 'e_y','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station','O_TIME', 'aver_v','max_v', 'Diff_Time']]\n",
    "print(col)\n",
    "X = train[col].values\n",
    "y = train['Diff_Time'].values\n",
    "print(X.shape)\n",
    "\n",
    "# line = line[(line['Diff_Time']<600)]\n",
    "# line['ID'] = line['ID'].astype('category')\n",
    "# line['s_ij'] = line['s_ij'].astype('category') \n",
    "# line['e_ij'] = line['e_ij'].astype('category') \n",
    "# line = shuffle(line)\n",
    "# train_num = int(0.9 * line.shape[0])\n",
    "# train = line[:train_num]\n",
    "# #!\n",
    "# dev = line[train_num:]\n",
    "# print(train.shape)\n",
    "# print(dev.shape)\n",
    "with open('./data/test-id4-crowd-grid_hadxu.txt', 'rb') as data_file:\n",
    "    test = pickle.load(data_file)\n",
    "test = zuhe(test)\n",
    "\n",
    "col1 = [c for c in test if\n",
    "       c not in ['Unnamed: 0','ID2' , 's_x', 's_y', 'e_x', 'e_y','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station', 'O_TIME', 'aver_v', 'max_v',\n",
    "                 'Diff_Time','Distance1', 'distance2','TERMINALNO', 'new_dist']]\n",
    "print(col1)\n",
    "X_test = test[col1].values\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(X,y,X_test,n_folds):\n",
    "    \n",
    "    def mse_v2(y_pred, train_data):\n",
    "        y_true = train_data.get_label()\n",
    "        return 'rmse', 600*(np.mean((y_pred-y_true)**2))**0.5, False\n",
    "    \n",
    "    res = np.zeros((len(X_test), n_folds))\n",
    "    \n",
    "    from sklearn.model_selection import StratifiedKFold, KFold\n",
    "    print(\"folding\")\n",
    "    kf = KFold(n_splits=n_folds,shuffle=True,random_state=2018)\n",
    "    result = np.zeros((len(X_test), 1))\n",
    "    print(\"training\")\n",
    "    count = 0\n",
    "    for (tr_idx, val_idx) in kf.split(y):\n",
    "        X_train = X[tr_idx]\n",
    "        y_train = y[tr_idx]\n",
    "\n",
    "        X_dev = X[val_idx]\n",
    "        y_dev = y[val_idx]\n",
    "        \n",
    "        print(X_train.shape)\n",
    "        print(X_dev.shape)\n",
    "        \n",
    "        lgb_train  = lgb.Dataset(X_train, y_train,)#feature_name=col,categorical_feature=['ID']\n",
    "        lgb_eval = lgb.Dataset(X_dev, y_dev,reference=lgb_train)\n",
    "        params = {'num_leaves':60, \n",
    "                  'max_depth':8,\n",
    "                  'seed':2018,\n",
    "                  'colsample_bytree':0.8,\n",
    "                  'subsample':0.9,\n",
    "                  'num_threads':4,\n",
    "                  'n_estimators':25000,\n",
    "                  'learning_rate': 0.1,\n",
    "                  'objective':'regression_l2',  \n",
    "                  # 'objective': 'xentropy',\n",
    "                  'metric':'rmse',\n",
    "                'device_type':'gpu',\n",
    "                 }\n",
    "        print(count)\n",
    "        gbm = lgb.train(params, lgb_train,early_stopping_rounds=200,valid_sets=lgb_eval,\n",
    "                        verbose_eval=50,\n",
    "                       # feval=mse_v2\n",
    "                       )\n",
    "\n",
    "        resultx = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "        resultx = np.reshape(resultx,(490380, 1))\n",
    "        print(resultx.shape,result.shape)\n",
    "        \n",
    "        res[:, count] = resultx[:, 0]\n",
    "        \n",
    "        result = result + resultx\n",
    "        y_te_pred = gbm.predict(X_dev, num_iteration=gbm.best_iteration)\n",
    "        #print(log_loss(y_dev, y_te_pred))\n",
    "\n",
    "        count = count+1\n",
    "    # 提交结果\n",
    "    result /= n_folds\n",
    "    return result, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_1cv(X,y,X_test):\n",
    "    \n",
    "    def mse_v2(y_pred, train_data):\n",
    "        y_true = train_data.get_label()\n",
    "        return 'rmse', 600*(np.mean((y_pred-y_true)**2))**0.5, False\n",
    "    \n",
    "    result = np.zeros((len(X_test), 1))\n",
    "    print(\"training\")\n",
    "    count = 0\n",
    "    \n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_dev, y_train, y_dev = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_dev.shape)\n",
    "        \n",
    "    lgb_train  = lgb.Dataset(X_train, y_train,)#feature_name=col,categorical_feature=['ID']\n",
    "    lgb_eval = lgb.Dataset(X_dev, y_dev,reference=lgb_train)\n",
    "    params = {'num_leaves':60, \n",
    "              'max_depth':8,\n",
    "              'seed':2018,\n",
    "              'colsample_bytree':0.8,\n",
    "              'subsample':0.9,\n",
    "              'num_threads':4,\n",
    "              'n_estimators':20000,\n",
    "              'learning_rate': 0.1,\n",
    "              'objective':'regression_l2',  \n",
    "              # 'objective': 'xentropy',\n",
    "              'metric':'rmse',\n",
    "            'device_type':'gpu',\n",
    "             }\n",
    "    gbm = lgb.train(params, lgb_train,early_stopping_rounds=200,valid_sets=lgb_eval,\n",
    "                        verbose_eval=50,\n",
    "                       # feval=mse_v2\n",
    "                       )\n",
    "\n",
    "    resultx = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n",
    "    resultx = np.reshape(resultx,(490380, 1))\n",
    "    print(resultx.shape,result.shape)\n",
    "    result = result + resultx\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "(10986842, 25)\n",
      "(2746711, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/.local/lib/python3.6/site-packages/lightgbm/engine.py:105: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's rmse: 39.7753\n",
      "[100]\tvalid_0's rmse: 39.0458\n",
      "[150]\tvalid_0's rmse: 38.7849\n",
      "[200]\tvalid_0's rmse: 38.629\n",
      "[250]\tvalid_0's rmse: 38.5078\n",
      "[300]\tvalid_0's rmse: 38.4036\n",
      "[350]\tvalid_0's rmse: 38.3226\n",
      "[400]\tvalid_0's rmse: 38.2337\n",
      "[450]\tvalid_0's rmse: 38.1577\n",
      "[500]\tvalid_0's rmse: 38.0921\n",
      "[550]\tvalid_0's rmse: 38.0391\n",
      "[600]\tvalid_0's rmse: 37.9957\n",
      "[650]\tvalid_0's rmse: 37.9447\n",
      "[700]\tvalid_0's rmse: 37.8938\n",
      "[750]\tvalid_0's rmse: 37.8498\n",
      "[800]\tvalid_0's rmse: 37.81\n",
      "[850]\tvalid_0's rmse: 37.7671\n",
      "[900]\tvalid_0's rmse: 37.738\n",
      "[950]\tvalid_0's rmse: 37.6957\n",
      "[1000]\tvalid_0's rmse: 37.6625\n",
      "[1050]\tvalid_0's rmse: 37.6277\n",
      "[1100]\tvalid_0's rmse: 37.5952\n",
      "[1150]\tvalid_0's rmse: 37.5608\n",
      "[1200]\tvalid_0's rmse: 37.5302\n",
      "[1250]\tvalid_0's rmse: 37.5025\n",
      "[1300]\tvalid_0's rmse: 37.4797\n",
      "[1350]\tvalid_0's rmse: 37.4512\n",
      "[1400]\tvalid_0's rmse: 37.4213\n",
      "[1450]\tvalid_0's rmse: 37.393\n",
      "[1500]\tvalid_0's rmse: 37.3662\n",
      "[1550]\tvalid_0's rmse: 37.3466\n",
      "[1600]\tvalid_0's rmse: 37.3232\n",
      "[1650]\tvalid_0's rmse: 37.3026\n",
      "[1700]\tvalid_0's rmse: 37.282\n",
      "[1750]\tvalid_0's rmse: 37.2594\n",
      "[1800]\tvalid_0's rmse: 37.2398\n",
      "[1850]\tvalid_0's rmse: 37.2203\n",
      "[1900]\tvalid_0's rmse: 37.2037\n",
      "[1950]\tvalid_0's rmse: 37.1848\n",
      "[2000]\tvalid_0's rmse: 37.1678\n",
      "[2050]\tvalid_0's rmse: 37.149\n",
      "[2100]\tvalid_0's rmse: 37.1375\n",
      "[2150]\tvalid_0's rmse: 37.1233\n"
     ]
    }
   ],
   "source": [
    "pred = train_1cv(X,y,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "folding\n",
      "training\n",
      "(12360197, 25)\n",
      "(1373356, 25)\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/.local/lib/python3.6/site-packages/lightgbm/engine.py:105: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds.\n",
      "[50]\tvalid_0's rmse: 39.7144\n",
      "[100]\tvalid_0's rmse: 38.9846\n",
      "[150]\tvalid_0's rmse: 38.7258\n",
      "[200]\tvalid_0's rmse: 38.5834\n",
      "[250]\tvalid_0's rmse: 38.4587\n",
      "[300]\tvalid_0's rmse: 38.338\n",
      "[350]\tvalid_0's rmse: 38.2422\n",
      "[400]\tvalid_0's rmse: 38.1602\n",
      "[450]\tvalid_0's rmse: 38.0905\n",
      "[500]\tvalid_0's rmse: 38.0255\n",
      "[550]\tvalid_0's rmse: 37.9699\n",
      "[600]\tvalid_0's rmse: 37.9111\n",
      "[650]\tvalid_0's rmse: 37.8529\n",
      "[700]\tvalid_0's rmse: 37.8092\n",
      "[750]\tvalid_0's rmse: 37.762\n",
      "[800]\tvalid_0's rmse: 37.7143\n",
      "[850]\tvalid_0's rmse: 37.6723\n",
      "[900]\tvalid_0's rmse: 37.6387\n",
      "[950]\tvalid_0's rmse: 37.6008\n",
      "[1000]\tvalid_0's rmse: 37.5703\n",
      "[1050]\tvalid_0's rmse: 37.5346\n",
      "[1100]\tvalid_0's rmse: 37.5022\n",
      "[1150]\tvalid_0's rmse: 37.4757\n",
      "[1200]\tvalid_0's rmse: 37.4526\n",
      "[1250]\tvalid_0's rmse: 37.4311\n",
      "[1300]\tvalid_0's rmse: 37.4073\n",
      "[1350]\tvalid_0's rmse: 37.3806\n",
      "[1400]\tvalid_0's rmse: 37.3478\n",
      "[1450]\tvalid_0's rmse: 37.3193\n",
      "[1500]\tvalid_0's rmse: 37.2951\n",
      "[1550]\tvalid_0's rmse: 37.2705\n",
      "[1600]\tvalid_0's rmse: 37.2498\n",
      "[1650]\tvalid_0's rmse: 37.2274\n",
      "[1700]\tvalid_0's rmse: 37.2048\n",
      "[1750]\tvalid_0's rmse: 37.1813\n",
      "[1800]\tvalid_0's rmse: 37.1606\n",
      "[1850]\tvalid_0's rmse: 37.1374\n",
      "[1900]\tvalid_0's rmse: 37.1144\n",
      "[1950]\tvalid_0's rmse: 37.0958\n",
      "[2000]\tvalid_0's rmse: 37.0765\n",
      "[2050]\tvalid_0's rmse: 37.0625\n",
      "[2100]\tvalid_0's rmse: 37.0482\n",
      "[2150]\tvalid_0's rmse: 37.0267\n",
      "[2200]\tvalid_0's rmse: 37.0093\n",
      "[2250]\tvalid_0's rmse: 36.9918\n",
      "[2300]\tvalid_0's rmse: 36.9788\n",
      "[2350]\tvalid_0's rmse: 36.9612\n",
      "[2400]\tvalid_0's rmse: 36.9452\n",
      "[2450]\tvalid_0's rmse: 36.9282\n",
      "[2500]\tvalid_0's rmse: 36.912\n",
      "[2550]\tvalid_0's rmse: 36.9005\n",
      "[2600]\tvalid_0's rmse: 36.8895\n",
      "[2650]\tvalid_0's rmse: 36.879\n",
      "[2700]\tvalid_0's rmse: 36.8665\n",
      "[2750]\tvalid_0's rmse: 36.8513\n",
      "[2800]\tvalid_0's rmse: 36.8405\n",
      "[2850]\tvalid_0's rmse: 36.8284\n",
      "[2900]\tvalid_0's rmse: 36.8206\n",
      "[2950]\tvalid_0's rmse: 36.8085\n",
      "[3000]\tvalid_0's rmse: 36.7972\n",
      "[3050]\tvalid_0's rmse: 36.7837\n",
      "[3100]\tvalid_0's rmse: 36.7699\n",
      "[3150]\tvalid_0's rmse: 36.7569\n",
      "[3200]\tvalid_0's rmse: 36.743\n",
      "[3250]\tvalid_0's rmse: 36.7308\n",
      "[3300]\tvalid_0's rmse: 36.7187\n",
      "[3350]\tvalid_0's rmse: 36.7079\n",
      "[3400]\tvalid_0's rmse: 36.6946\n",
      "[3450]\tvalid_0's rmse: 36.6839\n",
      "[3500]\tvalid_0's rmse: 36.6748\n",
      "[3550]\tvalid_0's rmse: 36.6615\n",
      "[3600]\tvalid_0's rmse: 36.6484\n",
      "[3650]\tvalid_0's rmse: 36.6403\n",
      "[3700]\tvalid_0's rmse: 36.6298\n",
      "[3750]\tvalid_0's rmse: 36.6214\n",
      "[3800]\tvalid_0's rmse: 36.6115\n",
      "[3850]\tvalid_0's rmse: 36.603\n",
      "[3900]\tvalid_0's rmse: 36.5944\n",
      "[3950]\tvalid_0's rmse: 36.5895\n",
      "[4000]\tvalid_0's rmse: 36.5793\n",
      "[4050]\tvalid_0's rmse: 36.5691\n",
      "[4100]\tvalid_0's rmse: 36.5594\n",
      "[4150]\tvalid_0's rmse: 36.5493\n",
      "[4200]\tvalid_0's rmse: 36.5416\n",
      "[4250]\tvalid_0's rmse: 36.5328\n",
      "[4300]\tvalid_0's rmse: 36.5222\n",
      "[4350]\tvalid_0's rmse: 36.5154\n",
      "[4400]\tvalid_0's rmse: 36.5081\n",
      "[4450]\tvalid_0's rmse: 36.5006\n",
      "[4500]\tvalid_0's rmse: 36.4958\n",
      "[4550]\tvalid_0's rmse: 36.4884\n",
      "[4600]\tvalid_0's rmse: 36.4804\n",
      "[4650]\tvalid_0's rmse: 36.4749\n",
      "[4700]\tvalid_0's rmse: 36.4685\n",
      "[4750]\tvalid_0's rmse: 36.4596\n",
      "[4800]\tvalid_0's rmse: 36.4501\n",
      "[4850]\tvalid_0's rmse: 36.444\n",
      "[4900]\tvalid_0's rmse: 36.4391\n",
      "[4950]\tvalid_0's rmse: 36.4326\n",
      "[5000]\tvalid_0's rmse: 36.4264\n",
      "[5050]\tvalid_0's rmse: 36.421\n",
      "[5100]\tvalid_0's rmse: 36.4136\n",
      "[5150]\tvalid_0's rmse: 36.4103\n",
      "[5200]\tvalid_0's rmse: 36.4023\n",
      "[5250]\tvalid_0's rmse: 36.3949\n",
      "[5300]\tvalid_0's rmse: 36.3896\n",
      "[5350]\tvalid_0's rmse: 36.3852\n",
      "[5400]\tvalid_0's rmse: 36.3785\n",
      "[5450]\tvalid_0's rmse: 36.3733\n",
      "[5500]\tvalid_0's rmse: 36.3681\n",
      "[5550]\tvalid_0's rmse: 36.3629\n",
      "[5600]\tvalid_0's rmse: 36.3574\n",
      "[5650]\tvalid_0's rmse: 36.3507\n",
      "[5700]\tvalid_0's rmse: 36.345\n",
      "[5750]\tvalid_0's rmse: 36.3394\n",
      "[5800]\tvalid_0's rmse: 36.3354\n",
      "[5850]\tvalid_0's rmse: 36.3299\n",
      "[5900]\tvalid_0's rmse: 36.324\n",
      "[5950]\tvalid_0's rmse: 36.3168\n",
      "[6000]\tvalid_0's rmse: 36.3107\n",
      "[6050]\tvalid_0's rmse: 36.3043\n",
      "[6100]\tvalid_0's rmse: 36.2984\n",
      "[6150]\tvalid_0's rmse: 36.2941\n",
      "[6200]\tvalid_0's rmse: 36.2881\n",
      "[6250]\tvalid_0's rmse: 36.2824\n",
      "[6300]\tvalid_0's rmse: 36.2764\n",
      "[6350]\tvalid_0's rmse: 36.2718\n",
      "[6400]\tvalid_0's rmse: 36.2663\n",
      "[6450]\tvalid_0's rmse: 36.2617\n",
      "[6500]\tvalid_0's rmse: 36.2578\n",
      "[6550]\tvalid_0's rmse: 36.2532\n",
      "[6600]\tvalid_0's rmse: 36.25\n",
      "[6650]\tvalid_0's rmse: 36.2448\n",
      "[6700]\tvalid_0's rmse: 36.2411\n",
      "[6750]\tvalid_0's rmse: 36.236\n",
      "[6800]\tvalid_0's rmse: 36.2302\n",
      "[6850]\tvalid_0's rmse: 36.2243\n",
      "[6900]\tvalid_0's rmse: 36.2187\n",
      "[6950]\tvalid_0's rmse: 36.2137\n",
      "[7000]\tvalid_0's rmse: 36.2086\n",
      "[7050]\tvalid_0's rmse: 36.2053\n",
      "[7100]\tvalid_0's rmse: 36.2016\n",
      "[7150]\tvalid_0's rmse: 36.1965\n",
      "[7200]\tvalid_0's rmse: 36.1923\n",
      "[7250]\tvalid_0's rmse: 36.1883\n",
      "[7300]\tvalid_0's rmse: 36.1841\n",
      "[7350]\tvalid_0's rmse: 36.1801\n",
      "[7400]\tvalid_0's rmse: 36.175\n",
      "[7450]\tvalid_0's rmse: 36.1718\n",
      "[7500]\tvalid_0's rmse: 36.1692\n",
      "[7550]\tvalid_0's rmse: 36.1658\n",
      "[7600]\tvalid_0's rmse: 36.1612\n",
      "[7650]\tvalid_0's rmse: 36.1582\n",
      "[7700]\tvalid_0's rmse: 36.1544\n",
      "[7750]\tvalid_0's rmse: 36.1512\n",
      "[7800]\tvalid_0's rmse: 36.147\n",
      "[7850]\tvalid_0's rmse: 36.1422\n",
      "[7900]\tvalid_0's rmse: 36.1388\n",
      "[7950]\tvalid_0's rmse: 36.1341\n",
      "[8000]\tvalid_0's rmse: 36.1306\n",
      "[8050]\tvalid_0's rmse: 36.1262\n",
      "[8100]\tvalid_0's rmse: 36.1226\n",
      "[8150]\tvalid_0's rmse: 36.1199\n",
      "[8200]\tvalid_0's rmse: 36.117\n",
      "[8250]\tvalid_0's rmse: 36.1143\n",
      "[8300]\tvalid_0's rmse: 36.1107\n",
      "[8350]\tvalid_0's rmse: 36.1072\n",
      "[8400]\tvalid_0's rmse: 36.1027\n",
      "[8450]\tvalid_0's rmse: 36.0984\n",
      "[8500]\tvalid_0's rmse: 36.0951\n",
      "[8550]\tvalid_0's rmse: 36.0933\n",
      "[8600]\tvalid_0's rmse: 36.0886\n",
      "[8650]\tvalid_0's rmse: 36.0845\n",
      "[8700]\tvalid_0's rmse: 36.0816\n",
      "[8750]\tvalid_0's rmse: 36.0776\n",
      "[8800]\tvalid_0's rmse: 36.073\n",
      "[8850]\tvalid_0's rmse: 36.0689\n",
      "[8900]\tvalid_0's rmse: 36.0648\n",
      "[8950]\tvalid_0's rmse: 36.0606\n",
      "[9000]\tvalid_0's rmse: 36.0563\n",
      "[9050]\tvalid_0's rmse: 36.0528\n",
      "[9100]\tvalid_0's rmse: 36.0479\n",
      "[9150]\tvalid_0's rmse: 36.0416\n",
      "[9200]\tvalid_0's rmse: 36.039\n",
      "[9250]\tvalid_0's rmse: 36.036\n",
      "[9300]\tvalid_0's rmse: 36.031\n",
      "[9350]\tvalid_0's rmse: 36.0284\n",
      "[9400]\tvalid_0's rmse: 36.0258\n",
      "[9450]\tvalid_0's rmse: 36.0228\n",
      "[9500]\tvalid_0's rmse: 36.0205\n",
      "[9550]\tvalid_0's rmse: 36.018\n",
      "[9600]\tvalid_0's rmse: 36.0158\n",
      "[9650]\tvalid_0's rmse: 36.0134\n",
      "[9700]\tvalid_0's rmse: 36.0112\n",
      "[9750]\tvalid_0's rmse: 36.0079\n",
      "[9800]\tvalid_0's rmse: 36.004\n",
      "[9850]\tvalid_0's rmse: 35.9985\n",
      "[9900]\tvalid_0's rmse: 35.9959\n",
      "[9950]\tvalid_0's rmse: 35.9927\n",
      "[10000]\tvalid_0's rmse: 35.9889\n",
      "[10050]\tvalid_0's rmse: 35.987\n",
      "[10100]\tvalid_0's rmse: 35.9832\n",
      "[10150]\tvalid_0's rmse: 35.9806\n",
      "[10200]\tvalid_0's rmse: 35.9764\n",
      "[10250]\tvalid_0's rmse: 35.9738\n",
      "[10300]\tvalid_0's rmse: 35.9705\n",
      "[10350]\tvalid_0's rmse: 35.967\n",
      "[10400]\tvalid_0's rmse: 35.9644\n",
      "[10450]\tvalid_0's rmse: 35.9609\n",
      "[10500]\tvalid_0's rmse: 35.9587\n",
      "[10550]\tvalid_0's rmse: 35.9562\n",
      "[10600]\tvalid_0's rmse: 35.954\n",
      "[10650]\tvalid_0's rmse: 35.9514\n",
      "[10700]\tvalid_0's rmse: 35.9473\n",
      "[10750]\tvalid_0's rmse: 35.944\n",
      "[10800]\tvalid_0's rmse: 35.9413\n",
      "[10850]\tvalid_0's rmse: 35.9385\n",
      "[10900]\tvalid_0's rmse: 35.9362\n",
      "[10950]\tvalid_0's rmse: 35.9337\n",
      "[11000]\tvalid_0's rmse: 35.931\n",
      "[11050]\tvalid_0's rmse: 35.9282\n",
      "[11100]\tvalid_0's rmse: 35.9263\n",
      "[11150]\tvalid_0's rmse: 35.9233\n",
      "[11200]\tvalid_0's rmse: 35.9207\n",
      "[11250]\tvalid_0's rmse: 35.9182\n",
      "[11300]\tvalid_0's rmse: 35.9156\n",
      "[11350]\tvalid_0's rmse: 35.9129\n",
      "[11400]\tvalid_0's rmse: 35.9111\n",
      "[11450]\tvalid_0's rmse: 35.9087\n",
      "[11500]\tvalid_0's rmse: 35.9063\n",
      "[11550]\tvalid_0's rmse: 35.904\n",
      "[11600]\tvalid_0's rmse: 35.9016\n",
      "[11650]\tvalid_0's rmse: 35.8994\n",
      "[11700]\tvalid_0's rmse: 35.8975\n",
      "[11750]\tvalid_0's rmse: 35.8963\n",
      "[11800]\tvalid_0's rmse: 35.8954\n",
      "[11850]\tvalid_0's rmse: 35.8935\n",
      "[11900]\tvalid_0's rmse: 35.8917\n",
      "[11950]\tvalid_0's rmse: 35.8905\n",
      "[12000]\tvalid_0's rmse: 35.8886\n",
      "[12050]\tvalid_0's rmse: 35.8859\n",
      "[12100]\tvalid_0's rmse: 35.8847\n",
      "[12150]\tvalid_0's rmse: 35.8831\n",
      "[12200]\tvalid_0's rmse: 35.8809\n",
      "[12250]\tvalid_0's rmse: 35.879\n",
      "[12300]\tvalid_0's rmse: 35.8776\n",
      "[12350]\tvalid_0's rmse: 35.8765\n",
      "[12400]\tvalid_0's rmse: 35.8752\n",
      "[12450]\tvalid_0's rmse: 35.8739\n",
      "[12500]\tvalid_0's rmse: 35.8723\n",
      "[12550]\tvalid_0's rmse: 35.8718\n",
      "[12600]\tvalid_0's rmse: 35.8703\n",
      "[12650]\tvalid_0's rmse: 35.8683\n",
      "[12700]\tvalid_0's rmse: 35.8658\n",
      "[12750]\tvalid_0's rmse: 35.8634\n",
      "[12800]\tvalid_0's rmse: 35.8606\n",
      "[12850]\tvalid_0's rmse: 35.8587\n",
      "[12900]\tvalid_0's rmse: 35.857\n",
      "[12950]\tvalid_0's rmse: 35.8543\n",
      "[13000]\tvalid_0's rmse: 35.8531\n",
      "[13050]\tvalid_0's rmse: 35.8508\n",
      "[13100]\tvalid_0's rmse: 35.8487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13150]\tvalid_0's rmse: 35.8461\n",
      "[13200]\tvalid_0's rmse: 35.8446\n",
      "[13250]\tvalid_0's rmse: 35.8428\n",
      "[13300]\tvalid_0's rmse: 35.8412\n",
      "[13350]\tvalid_0's rmse: 35.8394\n",
      "[13400]\tvalid_0's rmse: 35.8365\n",
      "[13450]\tvalid_0's rmse: 35.8347\n",
      "[13500]\tvalid_0's rmse: 35.8325\n",
      "[13550]\tvalid_0's rmse: 35.83\n",
      "[13600]\tvalid_0's rmse: 35.828\n",
      "[13650]\tvalid_0's rmse: 35.8264\n",
      "[13700]\tvalid_0's rmse: 35.8233\n",
      "[13750]\tvalid_0's rmse: 35.8202\n",
      "[13800]\tvalid_0's rmse: 35.8177\n",
      "[13850]\tvalid_0's rmse: 35.8165\n",
      "[13900]\tvalid_0's rmse: 35.814\n",
      "[13950]\tvalid_0's rmse: 35.8113\n",
      "[14000]\tvalid_0's rmse: 35.8097\n",
      "[14050]\tvalid_0's rmse: 35.8072\n",
      "[14100]\tvalid_0's rmse: 35.8055\n",
      "[14150]\tvalid_0's rmse: 35.804\n",
      "[14200]\tvalid_0's rmse: 35.8031\n",
      "[14250]\tvalid_0's rmse: 35.8016\n",
      "[14300]\tvalid_0's rmse: 35.8003\n",
      "[14350]\tvalid_0's rmse: 35.7983\n",
      "[14400]\tvalid_0's rmse: 35.7965\n",
      "[14450]\tvalid_0's rmse: 35.7936\n",
      "[14500]\tvalid_0's rmse: 35.7925\n",
      "[14550]\tvalid_0's rmse: 35.7914\n",
      "[14600]\tvalid_0's rmse: 35.7907\n",
      "[14650]\tvalid_0's rmse: 35.7893\n",
      "[14700]\tvalid_0's rmse: 35.7874\n",
      "[14750]\tvalid_0's rmse: 35.7859\n",
      "[14800]\tvalid_0's rmse: 35.7851\n",
      "[14850]\tvalid_0's rmse: 35.7836\n",
      "[14900]\tvalid_0's rmse: 35.7808\n",
      "[14950]\tvalid_0's rmse: 35.779\n",
      "[15000]\tvalid_0's rmse: 35.7768\n",
      "[15050]\tvalid_0's rmse: 35.7753\n",
      "[15100]\tvalid_0's rmse: 35.7737\n",
      "[15150]\tvalid_0's rmse: 35.7726\n",
      "[15200]\tvalid_0's rmse: 35.7704\n",
      "[15250]\tvalid_0's rmse: 35.7688\n",
      "[15300]\tvalid_0's rmse: 35.7668\n",
      "[15350]\tvalid_0's rmse: 35.7655\n",
      "[15400]\tvalid_0's rmse: 35.7636\n",
      "[15450]\tvalid_0's rmse: 35.7623\n",
      "[15500]\tvalid_0's rmse: 35.7615\n",
      "[15550]\tvalid_0's rmse: 35.7595\n",
      "[15600]\tvalid_0's rmse: 35.7579\n",
      "[15650]\tvalid_0's rmse: 35.7561\n",
      "[15700]\tvalid_0's rmse: 35.7541\n",
      "[15750]\tvalid_0's rmse: 35.7529\n",
      "[15800]\tvalid_0's rmse: 35.7515\n",
      "[15850]\tvalid_0's rmse: 35.7504\n",
      "[15900]\tvalid_0's rmse: 35.7496\n",
      "[15950]\tvalid_0's rmse: 35.7496\n",
      "[16000]\tvalid_0's rmse: 35.7484\n",
      "[16050]\tvalid_0's rmse: 35.7471\n",
      "[16100]\tvalid_0's rmse: 35.7447\n",
      "[16150]\tvalid_0's rmse: 35.7441\n",
      "[16200]\tvalid_0's rmse: 35.743\n"
     ]
    }
   ],
   "source": [
    "pred, res = train2(X,y,X_test, n_folds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'Distance', 'hour', 'is_peek', 'weekday', 'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'h_aver_diff', 'h_aver_d', 'h_aver_v', 'is_crowd', 's_ij', 'e_ij', 'grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kawayi-4/anaconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "with open('./data/test-id4-crowd-grid4.txt', 'rb') as data_file:\n",
    "    test = pickle.load(data_file)\n",
    "test = zuhe(test)\n",
    "test = test[['ID', 'new_dist',  'O_LINENO', 'O_UP', 'Source_Station', 'Target_Station',\n",
    "       'Distance', 'distance2', 'O_TIME', 'hour', 'is_peek', 'weekday',\n",
    "       'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'max_v',\n",
    "       'h_aver_diff', 'h_aver_d', 'h_aver_v', 'TERMINALNO', \n",
    "       'is_crowd', 's_ij', 'e_ij','grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']]\n",
    "test.columns=['ID', 'Distance',  'O_LINENO', 'O_UP', 'Source_Station', 'Target_Station',\n",
    "       'Distance1', 'distance2', 'O_TIME', 'hour', 'is_peek', 'weekday',\n",
    "       'is_workday', 'dws', 'nws', 'dts', 'nts', 'is_rain', 'max_v',\n",
    "       'h_aver_diff', 'h_aver_d', 'h_aver_v', 'TERMINALNO', \n",
    "       'is_crowd', 's_ij', 'e_ij','grid_aver_diff', 'grid_aver_d', 'hour_weekday', 'hour_crowd', 'hour_s', 'hour_e']\n",
    "col1 = [c for c in test if\n",
    "       c not in ['Unnamed: 0','ID2' , 's_x', 's_y', 'e_x', 'e_y','O_LINENO', 'O_UP', 'Source_Station', 'Target_Station', 'O_TIME', 'aver_v', 'max_v',\n",
    "                 'Diff_Time','Distance1', 'distance2','TERMINALNO', 'new_dist']]\n",
    "print(col1)\n",
    "\n",
    "test['pred1'] = pred\n",
    "test['pred2'] = pred\n",
    "\n",
    "sub1 = test[['O_LINENO','TERMINALNO', 'O_UP','Source_Station','Target_Station','O_TIME','pred1','pred2','Distance','Distance1']]\n",
    "sub1['O_TIME'] = pd.to_datetime(sub1['O_TIME'],format='%Y-%m-%d %H:%M:%S')\n",
    "sub1.columns = ['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime','pred1','pred2','Distance','Distance1']\n",
    "sub1 = sub1.reset_index()\n",
    "del sub1['index']\n",
    "\n",
    "sub=pd.read_csv(\"./toBePredicted_0607_segment.csv\", sep=\",\")\n",
    "sub['realTime'] = pd.to_datetime(sub['realTime'],format='%Y-%m-%d %H:%M:%S')\n",
    "sub2 = sub[['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime','distance']]\n",
    "sub2=pd.merge(sub2,sub1,on=['LINE','TERMINALNO','UP','pred_start_stop_ID','pred_end_stop_ID','realTime'],how='left')\n",
    "\n",
    "\n",
    "sub2['div_dist'] = sub2['Distance'] / sub2['Distance1']\n",
    "sub2['new_pred'] = sub2['div_dist'] * sub2['pred1']\n",
    "\n",
    "import math\n",
    "for i in range(sub2.shape[0]):\n",
    "    s = sub2.iloc[i]\n",
    "    if math.isnan(s['div_dist']):\n",
    "        sub2.loc[i,'new_pred'] = s['pred1']\n",
    "        \n",
    "sub2.to_csv('./toBePredicted_0807_result.csv',sep=\",\",index=False)#0605 29.2467\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pd.read_csv('toBePredicted_0813y.csv')\n",
    "y = pd.read_csv('toBePredicted_0813x.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = x['new_pred'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(490380,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_x = np.random.randint(-3,3, shape)\n",
    "noise_y = np.random.randint(-2,2, shape)\n",
    "\n",
    "x['new_pred'] = (x['new_pred']+noise_x + y['new_pred']+noise_y) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.to_csv('hadxu_noise_emsemble.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
